# Sample Training Datasets

This directory contains sample training datasets generated by the Trajectory Synthetic Data Generator.

## ğŸ“ Dataset Files

### Q&A Samples
- `qa_simple_5.jsonl` - Simple Q&A pairs (beginner-level)
- `qa_medium_5.jsonl` - Medium complexity Q&A pairs
- `qa_complex_5.jsonl` - Complex Q&A pairs (advanced)

### Query Transformations
- `transformations_demo_60.jsonl` - 60 transformed queries (30Ã— expansion from 2 seeds)
- `expanded_queries_30x.jsonl` - Complete 30Ã— expansion examples

### Multi-Iteration Trajectories
- `trajectories_demo.jsonl` - Multi-iteration trajectory examples
- `cli_test_trajectories.jsonl` - CLI-generated trajectories
- `multi_iteration_test.jsonl` - Test trajectories with CALLâ†’CALLâ†’ANSWER patterns

## ï¿½ï¿½ Training Data Format

Each training example follows this structure:
```json
{
  "Q": "User query",
  "COT": "Chain of thought reasoning",
  "Tool Set": [
    {
      "name": "tool_name",
      "description": "Tool description",
      "parameters": {...}
    }
  ],
  "Decision": "CALL | ASK: clarification | ANSWER: response",
  "metadata": {
    "iteration": 0,
    "decision_type": "CALL"
  }
}
```

## ğŸ“Š Quality Metrics

Run quality analysis on any dataset:
```bash
# Validate format
python scripts/validate_format.py samples/trajectories_demo.jsonl

# Analyze quality
python scripts/analyze_quality.py samples/trajectories_demo.jsonl

# Check diversity
python scripts/check_diversity.py samples/transformations_demo_60.jsonl

# Overall summary
python scripts/summarize_dataset.py
```

## ğŸ” Evaluation Scripts

Located in `scripts/`:
- `validate_format.py` - Validates training data structure
- `analyze_quality.py` - Analyzes COT length, decision distribution
- `check_diversity.py` - Measures query variation and coverage
- `summarize_dataset.py` - Generates comprehensive dataset summary

## ğŸ’¡ Usage for SLM Fine-Tuning

These datasets are ready for fine-tuning Small Language Models:

1. **Format**: JSONL (one example per line)
2. **Fields**: Consistent field names from config.yaml
3. **Quality**: Validated for structure and content
4. **Diversity**: Multiple personas, complexity levels, tool variants

### Recommended Usage:
```python
# Load training data
import jsonlines

with jsonlines.open('samples/trajectories_demo.jsonl') as reader:
    training_data = list(reader)

# Use for fine-tuning your SLM
# Each example teaches: when to CALL tools, ASK questions, or ANSWER
```

## ï¿½ï¿½ Expansion Rates

- **Q&A Generation**: 1 document â†’ 5-10 Q&A pairs
- **Persona Transformation**: 1 query â†’ 5 variants (P1-P5)
- **Complexity Transformation**: 1 query â†’ 3 variants (Q-, Q, Q+)
- **Tool Data Variants**: 1 query â†’ 2 variants (correct/incorrect)
- **Total**: 1 seed query â†’ 30 training variants (5 Ã— 3 Ã— 2)

## ğŸ“ Training Objectives

These datasets teach SLMs to:
- âœ… Decide when to use tools vs. answer directly
- âœ… Chain multiple tool calls for complex queries
- âœ… Ask for clarification when needed
- âœ… Generate coherent chain-of-thought reasoning
- âœ… Handle different user personas and complexity levels

---

For more information, see the main project README.
